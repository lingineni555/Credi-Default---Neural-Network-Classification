{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h2>Predicting Credit Card Default using Classification Models | Machine Learning</h2>\n",
    "<h4>Models used Logistic Regression, CART, Random Forest, Neural Network</h4>\n",
    "Created by <a href=\"https://www.linkedin.com/in/linginenivishal/\"> Vishal Lingineni </a> <br>\n",
    "Hult International Business School<br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h2>Purpose of this Script</h2><br>\n",
    "This script is designed to analyze the <a href=\"https://www.kaggle.com/mlg-ulb/creditcardfraud\">Credit Card Dataset</a>, a popular learning dataset from Kaggle.\n",
    "<br>\n",
    "<h2>Analytical Objectives</h2><br>\n",
    "Make a prediction on who will default in future\n",
    "<br>\n",
    "\n",
    "<h2> Methodology </h2> <br>\n",
    "<li> Split dataset into training (60%), validation (20%) and test (20%).</li>\n",
    "<li> We will train several machine learning algorithms to predict credit card fraud. </li>\n",
    "<li> Select from the models you’ve validated the BEST model using the F1 criteria.</li>\n",
    "<li> Run a final test using the test dataset you reserved.</li> \n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Import packages and dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172786.0</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172787.0</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172788.0</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172788.0</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172792.0</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V1         V2        V3        V4        V5        V6  \\\n",
       "Time                                                                     \n",
       "0.0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "0.0        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "1.0       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "1.0       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "2.0       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...             ...        ...       ...       ...       ...       ...   \n",
       "172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "                V7        V8        V9       V10  ...       V21       V22  \\\n",
       "Time                                              ...                       \n",
       "0.0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "0.0      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "1.0       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "1.0       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "2.0       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "172786.0 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "172787.0  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "172788.0 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "172788.0 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "172792.0  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "               V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "Time                                                                           \n",
       "0.0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "0.0       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "1.0       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "1.0      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "2.0      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...            ...       ...       ...       ...       ...       ...     ...   \n",
       "172786.0  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "172787.0  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "172788.0 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "172788.0 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "172792.0  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "          Class  \n",
       "Time             \n",
       "0.0           0  \n",
       "0.0           0  \n",
       "1.0           0  \n",
       "1.0           0  \n",
       "2.0           0  \n",
       "...         ...  \n",
       "172786.0      0  \n",
       "172787.0      0  \n",
       "172788.0      0  \n",
       "172788.0      0  \n",
       "172792.0      0  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file='creditcard.csv'\n",
    "df_creditcard=pd.read_csv(csv_file, index_col=0)\n",
    "df_creditcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>User Defined Functions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute F1 Score for Assignment 3.b\n",
    "def f1_score(p, y):\n",
    "# Follow the formulas for truth table and confusion matrix from\n",
    "# https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "# Note: \n",
    "# p, and y must be arrays of 0, 1 of the same lengths\n",
    "\n",
    "    comment01=\"\"\"\n",
    "    # Confusion Matrix:\n",
    "\n",
    "          Actual\n",
    "    Pred|  P | N \n",
    "     P  | TP | FP\n",
    "     N  | FN | TN\n",
    "    \"\"\"\n",
    "\n",
    "    tp=np.sum(p&y) #true positive\n",
    "    tn=np.sum((1-p)&(1-y)) #true negative\n",
    "    fp=np.sum(p)-tp #false positive (type i)\n",
    "    fn=np.sum(1-p)-tn #false negative (type ii)\n",
    "    f1=2*tp/(2*tp+fp+fn) # F1 score\n",
    "    return f1, tp, sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and output some summary statistics\n",
    "\n",
    "def stat_summary(df_data, to_print):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    stat_count=len(df_data.index)\n",
    "    stat_mean=np.mean(df_data)\n",
    "    stat_std=np.std(df_data)\n",
    "    stat_max=np.max(df_data)\n",
    "    stat_90pct=np.percentile(df_data, 90)\n",
    "    stat_median=np.median(df_data)\n",
    "    stat_min=np.min(df_data)\n",
    "\n",
    "        \n",
    "    #return\n",
    "    df_stat_summary=pd.DataFrame({'count': len(df_data.index),\n",
    "                    'mean': np.mean(df_data, axis=0),\n",
    "                    'stdev': np.std(df_data, axis=0),\n",
    "                    'stat_max': np.max(df_data, axis=0),\n",
    "                    'stat_90pct': np.percentile(df_data, 90, axis=0),\n",
    "                    'stat_75pct': np.percentile(df_data, 75, axis=0),\n",
    "                    'stat_media': np.median(df_data, axis=0),\n",
    "                    'stat_25pct': np.percentile(df_data, 25, axis=0),\n",
    "                    'stat_10pct': np.percentile(df_data, 10, axis=0),\n",
    "                    'stat_min': np.min(df_data, axis=0)} ,\n",
    "                     index = df_data.columns)\n",
    "    \n",
    "    #print?\n",
    "    if to_print:\n",
    "        print(df_stat_summary)\n",
    "        \n",
    "    return df_stat_summary\n",
    "# end of function stat_summary\n",
    "\n",
    "\n",
    "#df_stat=stat_summary(df_creditcard, True)\n",
    "#df_stat.to_csv('ccsummary_out.csv') # this is the .csv output file.  open to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Handling Missing values </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_creditcard.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any missing values hence no need to for imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preparation of Data for Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Transforming the variable </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAWNElEQVR4nO3df5Bd5X3f8ffHyGAaGyNswVCJBhwrUytJY8MG5JBJY9wRgmYinMGpqCdoKFN1XDuBpk2L6z9I7fxhd9rYQ+KQUQyDYIgxwfagpICsYuJMJxhY25gfxljrHzFbGCQqjHE9gyv87R/32fhG7K5WYu8+++P9mrlzz/me55x9Ht3VZ86eXzdVhSRp4b2idwckaaUygCWpEwNYkjoxgCWpEwNYkjpZ1bsDC23z5s1111139e6GpJUl0xVX3B7wM88807sLkgSswACWpMXCAJakTkYawEm+neThJA8mGW+1k5LsSbK3va9u9SS5JslEkoeSnDm0nW2t/d4k24bqZ7XtT7R1pz3OIkmL0ULsAb+tqt5cVWNt/irg7qpaD9zd5gEuANa313bgWhgENnA1cA5wNnD1VGi3NtuH1ts8+uFI0vzocQhiC7CzTe8ELhqq31gDXwBOTHIqcD6wp6oOVNWzwB5gc1t2QlXdW4MHWtw4tC1JWvRGHcAFfDbJF5Nsb7VTquopgPZ+cquvBZ4YWney1WarT05Tf4kk25OMJxnfv3//yxySJM2PUV8HfG5VPZnkZGBPkq/N0na647d1FPWXFqt2ADsAxsbGfPybpEVhpHvAVfVke98HfIbBMdyn2+ED2vu+1nwSOG1o9XXAk4epr5umLklLwsgCOMlPJHnN1DSwCXgE2AVMXcmwDbi9Te8CLm1XQ2wEnmuHKHYDm5KsbiffNgG727Lnk2xsVz9cOrQtSVr0RnkI4hTgM+3KsFXAn1XVXUkeAG5NcjnwHeCdrf0dwIXABPAD4DKAqjqQ5IPAA63dB6rqQJt+N3ADcDxwZ3tJ0pKQlfaNGGNjYzU+Pt67G5JWFp8FIUmLiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUycgDOMkxSb6c5C/b/BlJ7kuyN8knkxzb6se1+Ym2/PShbbyv1R9Pcv5QfXOrTSS5atRjkaT5tBB7wFcAjw3Nfxj4SFWtB54FLm/1y4Fnq+qNwEdaO5JsALYCPwNsBv64hfoxwMeAC4ANwCWtrSQtCSMN4CTrgH8OfLzNBzgPuK012Qlc1Ka3tHna8re39luAW6rqhar6FjABnN1eE1X1zar6IXBLaytJS8Ko94A/CvxH4Edt/nXAd6vqYJufBNa26bXAEwBt+XOt/d/VD1lnpvpLJNmeZDzJ+P79+1/umCRpXowsgJP8KrCvqr44XJ6maR1m2ZHWX1qs2lFVY1U1tmbNmll6LUkLZ9UIt30u8GtJLgReBZzAYI/4xCSr2l7uOuDJ1n4SOA2YTLIKeC1wYKg+ZXidmeqStOiNbA+4qt5XVeuq6nQGJ9E+V1XvAu4BLm7NtgG3t+ldbZ62/HNVVa2+tV0lcQawHrgfeABY366qOLb9jF2jGo8kzbdR7gHP5D8BtyT5feDLwHWtfh1wU5IJBnu+WwGq6tEktwJfBQ4C76mqFwGSvBfYDRwDXF9Vjy7oSCTpZchgJ3PlGBsbq/Hx8d7dkLSyTHfOyjvhJKkXA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJamTkQVwklcluT/JV5I8muS/tPoZSe5LsjfJJ5Mc2+rHtfmJtvz0oW29r9UfT3L+UH1zq00kuWpUY5GkURjlHvALwHlV9fPAm4HNSTYCHwY+UlXrgWeBy1v7y4Fnq+qNwEdaO5JsALYCPwNsBv44yTFJjgE+BlwAbAAuaW0laUkYWQDXwPfb7Cvbq4DzgNtafSdwUZve0uZpy9+eJK1+S1W9UFXfAiaAs9troqq+WVU/BG5pbSVpSRjpMeC2p/ogsA/YA3wD+G5VHWxNJoG1bXot8ARAW/4c8Lrh+iHrzFSfrh/bk4wnGd+/f/98DE2SXraRBnBVvVhVbwbWMdhjfdN0zdp7Zlh2pPXp+rGjqsaqamzNmjWH77gkLYAFuQqiqr4L/BWwETgxyaq2aB3wZJueBE4DaMtfCxwYrh+yzkx1SVoSRnkVxJokJ7bp44F/BjwG3ANc3JptA25v07vaPG3556qqWn1ru0riDGA9cD/wALC+XVVxLIMTdbtGNR5Jmm+rDt/kqJ0K7GxXK7wCuLWq/jLJV4Fbkvw+8GXgutb+OuCmJBMM9ny3AlTVo0luBb4KHATeU1UvAiR5L7AbOAa4vqoeHeF4JGleZbCTuXKMjY3V+Ph4725IWlmmO2flnXCS1IsBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1MmcAjjJuXOpSZLmbq57wH84x5okaY5mfRhPkrcCvwisSfI7Q4tOYPAAHEnSUTrc09COBV7d2r1mqP49fvxISUnSUZg1gKvq88Dnk9xQVX+7QH2SpBVhrs8DPi7JDuD04XWq6rxRdEqSVoK5BvCfA38CfBx4cXTdkaSVY64BfLCqrh1pTyRphZnrZWh/keTfJjk1yUlTr5H2TJKWubnuAU99WebvDtUKeMP8dkeSVo45BXBVnTHqjkjSSjOnAE5y6XT1qrpxfrsjSSvHXA9B/MLQ9KuAtwNfAgxgSTpKcz0E8VvD80leC9w0kh5J0gpxtI+j/AGwfj47IkkrzVyPAf8Fg6seYPAQnjcBt46qU5K0Esz1GPB/G5o+CPxtVU2OoD+StGLM6RBEeyjP1xg8EW018MNRdkqSVoK5fiPGbwD3A+8EfgO4L4mPo5Skl2GuhyDeD/xCVe0DSLIG+J/AbaPqmCQtd3O9CuIVU+Hb/J8jWFeSNI257gHflWQ38Ik2/y+AO0bTJUlaGQ73nXBvBE6pqt9N8uvALwEB7gVuXoD+SdKydbjDCB8Fngeoqk9X1e9U1b9jsPf70VF3TpKWs8MF8OlV9dChxaoaZ/D1RJKko3S4AH7VLMuOn8+OSNJKc7gAfiDJvz60mORy4Iuj6ZIkrQyHuwriSuAzSd7FjwN3DDgWeMcoOyZJy92sAVxVTwO/mORtwM+28v+oqs+NvGeStMzN9XnA9wD3jLgvkrSieDebJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJyML4CSnJbknyWNJHk1yRauflGRPkr3tfXWrJ8k1SSaSPJTkzKFtbWvt9ybZNlQ/K8nDbZ1rkmRU45Gk+TbKPeCDwL+vqjcBG4H3JNkAXAXcXVXrgbvbPMAFwPr22g5cC4PABq4GzgHOBq6eCu3WZvvQeptHOB5JmlcjC+CqeqqqvtSmnwceA9YCW4CdrdlO4KI2vQW4sQa+AJyY5FTgfGBPVR2oqmeBPcDmtuyEqrq3qgq4cWhbkrToLcgx4CSnA28B7mPwNfdPwSCkgZNbs7XAE0OrTbbabPXJaerT/fztScaTjO/fv//lDkeS5sXIAzjJq4FPAVdW1fdmazpNrY6i/tJi1Y6qGquqsTVr1hyuy5K0IEYawEleySB8b66qT7fy0+3wAe19X6tPAqcNrb4OePIw9XXT1CVpSRjlVRABrgMeq6o/GFq0C5i6kmEbcPtQ/dJ2NcRG4Ll2iGI3sCnJ6nbybROwuy17PsnG9rMuHdqWJC16c/pOuKN0LvCbwMNJHmy1/wx8CLi1fbX9d4B3tmV3ABcCE8APgMsAqupAkg8CD7R2H6iqA2363cANwPHAne0lSUtCBhcQrBxjY2M1Pj7euxuSVpZp71HwTjhJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6mRkAZzk+iT7kjwyVDspyZ4ke9v76lZPkmuSTCR5KMmZQ+tsa+33Jtk2VD8rycNtnWuSZFRjkaRRGOUe8A3A5kNqVwF3V9V64O42D3ABsL69tgPXwiCwgauBc4CzgaunQru12T603qE/S5IWtZEFcFX9NXDgkPIWYGeb3glcNFS/sQa+AJyY5FTgfGBPVR2oqmeBPcDmtuyEqrq3qgq4cWhbkrQkLPQx4FOq6imA9n5yq68FnhhqN9lqs9Unp6lL0pKxWE7CTXf8to6iPv3Gk+1JxpOM79+//yi7KEnza6ED+Ol2+ID2vq/VJ4HThtqtA548TH3dNPVpVdWOqhqrqrE1a9a87EFI0nxY6ADeBUxdybANuH2ofmm7GmIj8Fw7RLEb2JRkdTv5tgnY3ZY9n2Rju/rh0qFtSdKSsGpUG07yCeBXgNcnmWRwNcOHgFuTXA58B3hna34HcCEwAfwAuAygqg4k+SDwQGv3gaqaOrH3bgZXWhwP3NlekrRkZHARwcoxNjZW4+PjvbshaWWZ9j6FxXISTpJWHANYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwN4DrbuuLd3FyQtQwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJ0s+gJNsTvJ4kokkV43q53gzhqT5tqQDOMkxwMeAC4ANwCVJNvTtlSTNzZIOYOBsYKKqvllVPwRuAbaM6odt3XGve8KS5s2q3h14mdYCTwzNTwLnHNooyXZge5v9fpLHj/DnvB54Zmrmk//mCNdePP7eOJa45TKW5TIOcCyzuauqNh9aXOoBnGlq9ZJC1Q5gx1H/kGS8qsaOdv3FYrmMA5bPWJbLOMCxHI2lfghiEjhtaH4d8GSnvkjSEVnqAfwAsD7JGUmOBbYCuzr3SZLmZEkfgqiqg0neC+wGjgGur6pHR/CjjvrwxSKzXMYBy2csy2Uc4FiOWKpecshUkrQAlvohCElasgxgSerEAJ7FQt3m/HIk+XaSh5M8mGS81U5KsifJ3va+utWT5Jo2noeSnDm0nW2t/d4k2xao79cn2ZfkkaHavPU9yVnt32airTvdZYujHMvvJfnf7bN5MMmFQ8ve1/r1eJLzh+rT/s61E833tTF+sp10HsU4TktyT5LHkjya5IpWX3KfyyxjWTyfS1X5mubF4KTeN4A3AMcCXwE29O7XNP38NvD6Q2r/FbiqTV8FfLhNXwjcyeD66Y3Afa1+EvDN9r66Ta9egL7/MnAm8Mgo+g7cD7y1rXMncMECj+X3gP8wTdsN7ffpOOCM9nt2zGy/c8CtwNY2/SfAu0c0jlOBM9v0a4Cvt/4uuc9llrEsms/FPeCZLehtzvNsC7CzTe8ELhqq31gDXwBOTHIqcD6wp6oOVNWzwB7gJXftzLeq+mvgwCj63padUFX31uB/x41D21qoscxkC3BLVb1QVd8CJhj8vk37O9f2EM8DbmvrD/+7zKuqeqqqvtSmnwceY3DH6ZL7XGYZy0wW/HMxgGc23W3Os314vRTw2SRfzOCWa4BTquopGPwSAie3+kxjWkxjna++r23Th9YX2nvbn+bXT/3ZzpGP5XXAd6vq4CH1kUpyOvAW4D6W+OdyyFhgkXwuBvDM5nSb8yJwblWdyeCJcO9J8suztJ1pTEthrEfa98UwpmuBnwLeDDwF/PdWX/RjSfJq4FPAlVX1vdmaTlNb7GNZNJ+LATyzJXGbc1U92d73AZ9h8OfS0+1PPdr7vtZ8pjEtprHOV98n2/Sh9QVTVU9X1YtV9SPgTxl8NnDkY3mGwZ/2qw6pj0SSVzIIrJur6tOtvCQ/l+nGspg+FwN4Zov+NuckP5HkNVPTwCbgEQb9nDrrvA24vU3vAi5tZ643As+1Pyd3A5uSrG5/jm1qtR7mpe9t2fNJNrZjdZcObWtBTAVW8w4Gnw0MxrI1yXFJzgDWMzgxNe3vXDtWeg9wcVt/+N9lvvsc4Drgsar6g6FFS+5zmWksi+pzGcXZx+XyYnCG9+sMzoC+v3d/punfGxickf0K8OhUHxkcm7ob2NveT2r1MHiA/TeAh4GxoW39KwYnHSaAyxao/59g8Cfg/2Owl3H5fPYdGGv/ub4B/BHtzs8FHMtNra8Ptf/cpw61f3/r1+MMXQUw0+9c+6zvb2P8c+C4EY3jlxj8Gf0Q8GB7XbgUP5dZxrJoPhdvRZakTjwEIUmdGMCS1IkBLEmdGMCS1IkBLEmdGMBa9pK8I0kl+ccd+3Blkn/Q6+drcTKAtRJcAvwvBhfQ93IlYADr7zGAtay15wCcy+DGiK2t9itJPp/k1iRfT/KhJO9Kcn97Tu1PtXY/meTu9tCWu5P8o1a/IcnFQz/j+0Pb/asktyX5WpKb2x1ivw38Q+CeJPcs8D+BFjEDWMvdRcBdVfV14EB+/MDwnweuAH4O+E3gp6vqbODjwG+1Nn/E4FGL/wS4GbhmDj/vLQz2djcwuEvq3Kq6hsEzAt5WVW+bn2FpOTCAtdxdwuD5rbT3S9r0AzV4XuwLDG4v/WyrPwyc3qbfCvxZm76Jwa2th3N/VU3W4EEvDw5tS3qJJf219NJskryOwQOzfzZJMfhmgwLuAF4YavqjofkfMfP/i6n79g/Sdl7aA1+Gv4ZmeLsvzrItyT1gLWsXMziE8JNVdXpVnQZ8i7ntyQL8DT8+cfcuBifyYPA1UGe16S3AK+ewrecZfC2O9HcMYC1nlzB4RvKwTwH/co7r/zZwWZKHGBwnvqLV/xT4p0nuB84B/u8ctrUDuNOTcBrm09AkqRP3gCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpk/8P/pZWaQvv7y8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data=df_creditcard, x='Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Note: </strong> <br>\n",
    "<li>Notice, V1-V28 and Amount are features.  Except for 'Amount', it appears that </li>\n",
    "<li>The source of the data had hidden the meaning of the features (other than 'Amount'). This may be due to privacy or confitentiality reasons. </li>\n",
    "<li>It is also evident that the 'Amount' will need to transformed into log and normalization.  </li>\n",
    "<li>Will first transform the Amount into log(Amount). </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_creditcard['Amount']=np.log(df_creditcard['Amount']+0.01) \n",
    "# we are adding this 0.01 because we have some values are 0 and log of 0 is infinite\n",
    "sns.displot(data=df_creditcard, x='Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train Test Split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will handle feature normalization AFTER we split the samples into train/validate/test sub samples:\n",
    "\n",
    "# let's first split the data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=np.array(df_creditcard.iloc[:,0:-1]) # get features from df, convert to array for easy numerical operation\n",
    "y=np.array(df_creditcard['Class']) # get target, note y=1 for bankruptcy, 0 for no bankruptcy\n",
    "\n",
    "# splitting into training, and test\n",
    "# note: use stratify as y=1 is rare (~3% of total observation). stratify ensures that positives are split proportionally\n",
    "Xtra, Xtest, ytra, ytest = train_test_split(X, y, stratify=y, test_size=0.20, random_state=0)\n",
    "# further split training into train and validation\n",
    "Xtra, Xval, ytra, yval = train_test_split(Xtra, ytra, stratify=ytra, test_size=0.25, random_state=0)\n",
    "# final split is 60:20:20, tra:val:test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalizing the data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mean     stdev\n",
      "0  -0.001962  1.958596\n",
      "1   0.000358  1.660070\n",
      "2   0.000663  1.510103\n",
      "3   0.001067  1.415569\n",
      "4   0.004293  1.372323\n",
      "5   0.002928  1.329652\n",
      "6   0.000213  1.220275\n",
      "7   0.003190  1.187730\n",
      "8  -0.000714  1.098283\n",
      "9   0.000635  1.084003\n",
      "10  0.000842  1.021678\n",
      "11 -0.001293  0.998748\n",
      "12 -0.001323  0.996591\n",
      "13  0.003388  0.956323\n",
      "14 -0.000740  0.916705\n",
      "15  0.000556  0.876172\n",
      "16 -0.000904  0.850386\n",
      "17 -0.000431  0.838549\n",
      "18 -0.001285  0.812988\n",
      "19  0.000882  0.767585\n",
      "20  0.001112  0.733840\n",
      "21  0.000570  0.725706\n",
      "22 -0.001494  0.633661\n",
      "23  0.000685  0.606592\n",
      "24 -0.000240  0.522203\n",
      "25 -0.001163  0.481035\n",
      "26  0.000313  0.401720\n",
      "27  0.000269  0.330026\n",
      "28  2.960233  1.950247\n"
     ]
    }
   ],
   "source": [
    "# Now lets normalize features\n",
    "\n",
    "from sklearn import preprocessing\n",
    "x_scaler = preprocessing.StandardScaler().fit(Xtra)\n",
    "\n",
    "print(pd.DataFrame({'mean': x_scaler.mean_, 'stdev': x_scaler.scale_}))\n",
    "Xtra = x_scaler.transform(Xtra)\n",
    "Xval = x_scaler.transform(Xval) \n",
    "Xtest = x_scaler.transform(Xtest) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Classification Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CART (Classification Tree) </h3>\n",
    "Here you should fit the training data with a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):   1.0\n",
      "validation score (accuracy):   0.9992275552122467\n",
      "training score (F1, tp, p):   (1.0, 295, 295)\n",
      "validation score (F1, tp, p):   (0.7821782178217822, 79, 99)\n"
     ]
    }
   ],
   "source": [
    "# Here, instead of 'DecisionTreeRegressor' in Class 2, we will use 'DecisionTreeClassifier'.\n",
    "# This is because our target variable is binary (0,1), for fraud =1, and valid =0 transactions.\n",
    "# 'DecisionTreeRegressor' is for regression of a continuous dependent variable, while\n",
    "# 'DecisionTreeClassifier' is for discrete classification variable, including binary.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fill in and REPLACE the blanks ____\n",
    "# Hint: follow example from Class 2, but change the tree from Regressor to Classifier\n",
    "lonetree = DecisionTreeClassifier(random_state=0).fit(Xtra, ytra)\n",
    "\n",
    "print('training score (accuracy):  ', lonetree.score(Xtra,ytra))\n",
    "print('validation score (accuracy):  ', lonetree.score(Xval,yval))\n",
    "print('training score (F1, tp, p):  ', f1_score(lonetree.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, p):  ', f1_score(lonetree.predict(Xval),yval))\n",
    "\n",
    "lonetree_F1 = f1_score(lonetree.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7657142857142857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest </h3>\n",
    "That tree is a bit lonely.  Let's be nice and give it an ensemble of trees: a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):   1.0\n",
      "validation score (accuracy):   0.9995962220427653\n",
      "training score (F1, tp, p):   (1.0, 295, 295)\n",
      "validation score (F1, tp, p):   (0.8756756756756757, 81, 99)\n"
     ]
    }
   ],
   "source": [
    "# Here, instead of 'RandomForestRegressor' in Class 2, we will use 'RandomForestClassifier'.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fill in and REPLACE the blanks ____\n",
    "# Hint: follow example from Class 2_use the forest version, but replace Regressor with Classifier\n",
    "#   note: set n_estimators=100, and set the random_state to any integer\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0).fit(Xtra, ytra)\n",
    "\n",
    "print('training score (accuracy):  ', forest.score(Xtra,ytra))\n",
    "print('validation score (accuracy):  ', forest.score(Xval,yval))\n",
    "print('training score (F1, tp, p):  ', f1_score(forest.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, p):  ', f1_score(forest.predict(Xval),yval))\n",
    "Random_Forest_F1 = f1_score(forest.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logistic Regression </h3> <br>\n",
    "Were you able to improve your forcasting accuracy or f1 score with random forest? \n",
    "\n",
    "Now let's try logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):   0.9992041338225569\n",
      "validation score (accuracy):   0.9992802219023208\n",
      "training score (F1, tp, p):   (0.728, 182, 295)\n",
      "validation score (F1, tp, p):   (0.7657142857142857, 67, 99)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# Fill in and REPLACE the blanks ____\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit=LogisticRegression(random_state=0).fit(Xtra, ytra)\n",
    "\n",
    "print('training score (accuracy):  ', logit.score(Xtra,ytra))\n",
    "print('validation score (accuracy):  ', logit.score(Xval,yval))\n",
    "print('training score (F1, tp, p):  ', f1_score(logit.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, p):  ', f1_score(logit.predict(Xval),yval))\n",
    "logit_F1 = f1_score(logit.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network (Deep Learning) </h3>\n",
    "Finally, let's train a neural network to see if we can improve on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):   0.9996020669112785\n",
      "validation score (accuracy):   0.9995259997893332\n",
      "training score (F1, tp, pos):   (0.8754578754578755, 239, 295)\n",
      "validation score (F1, tp, pos):   (0.8586387434554974, 82, 99)\n"
     ]
    }
   ],
   "source": [
    "# First, try a neural network with \n",
    "# (solver='adam', activation='tanh')\n",
    "#  And use the following parameter settings.\n",
    "\n",
    "alpha_value=0.01\n",
    "random_seed=0\n",
    "hidden_layer=[10, 10]\n",
    "max_itervalue=1000\n",
    "# You may need to increase max_iter if the solution does not converge--sklearn will warn you.\n",
    "\n",
    "# Fill in and REPLACE the blanks ____\n",
    "\n",
    "nn_01 = MLPClassifier(solver='adam', activation='tanh',max_iter=max_itervalue,\n",
    "                      alpha=alpha_value, random_state=random_seed, \n",
    "                      hidden_layer_sizes= hidden_layer).fit(Xtra,ytra)\n",
    "\n",
    "print('training score (accuracy):  ', nn_01.score(Xtra,ytra))\n",
    "print('validation score (accuracy):  ', nn_01.score(Xval,yval))\n",
    "print('training score (F1, tp, pos):  ', f1_score(nn_01.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, pos):  ', f1_score(nn_01.predict(Xval),yval))\n",
    "NeuralNet_F1 = f1_score(nn_01.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):  0.9996664384403364\n",
      "validation score (accuracy):  0.9996137776061234\n",
      "training score (F1, tp, pos):  (0.8980322003577818, 251, 295)\n",
      "validation score (F1, tp, pos):  (0.8888888888888888, 88, 99)\n"
     ]
    }
   ],
   "source": [
    "# Now try different hidden layer setup. Perhaps with one of these:\n",
    "#hidden_layer=[16, 8, 4]\n",
    "# or\n",
    "hidden_layer=[32, 16, 8, 4]\n",
    "# or\n",
    "#hidden_layer=[10,10,10]\n",
    "\n",
    "# complete code to estimate the a nn_02 following the previous cell.  \n",
    "# And print the scores of nn02.\n",
    "\n",
    "nn_02= MLPClassifier(solver='adam', activation='tanh', max_iter = 1000, alpha = 0.01,\n",
    "                    random_state=0, hidden_layer_sizes=hidden_layer).fit(Xtra,ytra)\n",
    "\n",
    "print('training score (accuracy): ', nn_02.score(Xtra,ytra))\n",
    "print('validation score (accuracy): ', nn_02.score(Xval,yval))\n",
    "print('training score (F1, tp, pos): ', f1_score(nn_02.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, pos): ', f1_score(nn_02.predict(Xval),yval))\n",
    "NeuralNet_Best_F1 = f1_score(nn_02.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):  0.9997308099693942\n",
      "validation score (accuracy):  0.9995962220427653\n",
      "training score (F1, tp, pos):  (0.9166666666666666, 253, 295)\n",
      "validation score (F1, tp, pos):  (0.882051282051282, 86, 99)\n"
     ]
    }
   ],
   "source": [
    "# Now try different hidden layer setup. Perhaps with one of these:\n",
    "hidden_layer=[16, 8, 4]\n",
    "# or\n",
    "#hidden_layer=[32, 16, 8, 4]\n",
    "# or\n",
    "#hidden_layer=[10,10,10]\n",
    "\n",
    "# complete code to estimate the a nn_02 following the previous cell.  \n",
    "# And print the scores of nn02.\n",
    "\n",
    "nn_03= MLPClassifier(solver='adam', activation='tanh', max_iter = 1000, alpha = 0.01,\n",
    "                    random_state=0, hidden_layer_sizes=hidden_layer).fit(Xtra,ytra)\n",
    "\n",
    "print('training score (accuracy): ', nn_03.score(Xtra,ytra))\n",
    "print('validation score (accuracy): ', nn_03.score(Xval,yval))\n",
    "print('training score (F1, tp, pos): ', f1_score(nn_03.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, pos): ', f1_score(nn_03.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):  0.999502583639098\n",
      "validation score (accuracy):  0.9995962220427653\n",
      "training score (F1, tp, pos):  (0.8374760994263862, 219, 295)\n",
      "validation score (F1, tp, pos):  (0.8756756756756757, 81, 99)\n"
     ]
    }
   ],
   "source": [
    "# Now try different hidden layer setup. Perhaps with one of these:\n",
    "#hidden_layer=[16, 8, 4]\n",
    "# or\n",
    "#hidden_layer=[32, 16, 8, 4]\n",
    "# or\n",
    "hidden_layer=[10,10,10]\n",
    "\n",
    "# complete code to estimate the a nn_02 following the previous cell.  \n",
    "# And print the scores of nn02.\n",
    "\n",
    "nn_04= MLPClassifier(solver='adam', activation='tanh', max_iter = 1000, alpha = 0.01,\n",
    "                    random_state=0, hidden_layer_sizes=hidden_layer).fit(Xtra,ytra)\n",
    "\n",
    "print('training score (accuracy): ', nn_04.score(Xtra,ytra))\n",
    "print('validation score (accuracy): ', nn_04.score(Xval,yval))\n",
    "print('training score (F1, tp, pos): ', f1_score(nn_04.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, pos): ', f1_score(nn_04.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer=[32, 16, 8, 4] is producing the highest F1 score of 88.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>CART(Lone Tree)</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765714</td>\n",
       "      <td>0.782178</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Logistic Regression  CART(Lone Tree)  Random Forest  Neural Network\n",
       "0             0.765714         0.782178       0.875676        0.888889"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_results = pd.DataFrame(data = {\n",
    "    'Logistic Regression'   : logit_F1,\n",
    "    'CART(Lone Tree)'       : lonetree_F1,\n",
    "    'Random Forest'         : Random_Forest_F1,\n",
    "    'Neural Network'        : NeuralNet_Best_F1,\n",
    "    },index=[0])\n",
    "\n",
    "prediction_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network has the highest F1 Score, Lets hyper tune it to see if the accuracy increases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tuning Regularization Parameter of Neural Network </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tunning alpha value </h4> \n",
    "Find the best regularization parameter value alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc_tra</th>\n",
       "      <th>Acc_val</th>\n",
       "      <th>F1_tra</th>\n",
       "      <th>F1_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.934744</td>\n",
       "      <td>0.873096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0003</th>\n",
       "      <td>0.999854</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.867347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999508</td>\n",
       "      <td>0.917266</td>\n",
       "      <td>0.849462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0030</th>\n",
       "      <td>0.999789</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.898032</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0300</th>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.858156</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1000</th>\n",
       "      <td>0.999397</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>0.808905</td>\n",
       "      <td>0.817204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3000</th>\n",
       "      <td>0.998274</td>\n",
       "      <td>0.998262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.998274</td>\n",
       "      <td>0.998262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0000</th>\n",
       "      <td>0.998274</td>\n",
       "      <td>0.998262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Acc_tra   Acc_val    F1_tra    F1_val\n",
       "0.0001  0.999783  0.999561  0.934744  0.873096\n",
       "0.0003  0.999854  0.999544  0.956971  0.867347\n",
       "0.0010  0.999731  0.999508  0.917266  0.849462\n",
       "0.0030  0.999789  0.999526  0.935252  0.857143\n",
       "0.0100  0.999666  0.999614  0.898032  0.888889\n",
       "0.0300  0.999532  0.999614  0.858156  0.888889\n",
       "0.1000  0.999397  0.999403  0.808905  0.817204\n",
       "0.3000  0.998274  0.998262         0         0\n",
       "1.0000  0.998274  0.998262         0         0\n",
       "3.0000  0.998274  0.998262         0         0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow the example in Assignment 3 to tune alpha:\n",
    "# Must complete this cell, includng output the df table that shows the accuracy and F1 scores\n",
    "# for training and validation for each iteration!\n",
    "\n",
    "# iterate through alpha in a range and collect scores, and print at the end. \n",
    "alpharange=[0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "df_accuracy=pd.DataFrame([], index=alpharange, columns=['Acc_tra','Acc_val','F1_tra','F1_val'])\n",
    "for a in alpharange:\n",
    "    # complete the code to fit MLPClassifier in this for loop\n",
    "    nn_05 = MLPClassifier(solver='adam', activation='tanh', \n",
    "                      max_iter=1000, alpha=a, random_state=0, \n",
    "                      hidden_layer_sizes=[32, 16, 8, 4]).fit(Xtra, ytra) \n",
    "    # collect results, i.e. prediction scores\n",
    "    df_accuracy.loc[a,:]=[nn_05.score(Xtra,ytra),nn_05.score(Xval,yval), \n",
    "                          f1_score(nn_05.predict(Xtra),ytra)[0], f1_score(nn_05.predict(Xval),yval)[0]]\n",
    "    \n",
    "df_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha = 0.01 is having the highest F1 score of 88.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tunning Random State </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc_tra</th>\n",
       "      <th>Acc_val</th>\n",
       "      <th>F1_tra</th>\n",
       "      <th>F1_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.898032</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.883249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.89317</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>0.875676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.868293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.859885</td>\n",
       "      <td>0.837989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.909747</td>\n",
       "      <td>0.873684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999508</td>\n",
       "      <td>0.999456</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.845771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.885572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.92029</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.916968</td>\n",
       "      <td>0.901554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.999508</td>\n",
       "      <td>0.881041</td>\n",
       "      <td>0.844444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.99969</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.905526</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.898182</td>\n",
       "      <td>0.87234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.911032</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.877442</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.928826</td>\n",
       "      <td>0.902564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.883495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.902527</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.891986</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.897163</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.891986</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.885662</td>\n",
       "      <td>0.887755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.90087</td>\n",
       "      <td>0.864322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.999649</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.902564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.999649</td>\n",
       "      <td>0.91042</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.909747</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.902998</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.99969</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.858696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Acc_tra   Acc_val    F1_tra    F1_val\n",
       "0   0.999666  0.999614  0.898032  0.888889\n",
       "1   0.999666  0.999596  0.896926  0.883249\n",
       "2   0.999643  0.999473   0.89317  0.854369\n",
       "3   0.999684  0.999596  0.900369  0.875676\n",
       "4   0.999614  0.999526  0.886598  0.868293\n",
       "5   0.999573  0.999491  0.859885  0.837989\n",
       "6   0.999707  0.999579  0.909747  0.873684\n",
       "7   0.999508  0.999456  0.853147  0.845771\n",
       "8   0.999661  0.999596  0.897887  0.885572\n",
       "9   0.999743  0.999631   0.92029  0.888889\n",
       "10  0.999731  0.999666  0.916968  0.901554\n",
       "11  0.999625  0.999508  0.881041  0.844444\n",
       "12   0.99969  0.999544  0.905526      0.87\n",
       "13  0.999672  0.999579  0.898182   0.87234\n",
       "14  0.999707  0.999579  0.911032      0.88\n",
       "15  0.999596  0.999614  0.877442      0.89\n",
       "16  0.999766  0.999666  0.928826  0.902564\n",
       "17  0.999672  0.999561       0.9  0.871795\n",
       "18  0.999561  0.999579  0.871795  0.883495\n",
       "19  0.999684  0.999579  0.902527  0.878788\n",
       "20  0.999637  0.999526  0.891986  0.869565\n",
       "21  0.999661  0.999579  0.897163  0.882353\n",
       "22  0.999637  0.999579  0.891986  0.878788\n",
       "23  0.999631  0.999614  0.885662  0.887755\n",
       "24  0.999666  0.999526   0.90087  0.864322\n",
       "25  0.999649  0.999666  0.890909  0.902564\n",
       "26  0.999713  0.999649   0.91042  0.893617\n",
       "27  0.999707  0.999631  0.909747  0.892308\n",
       "28  0.999678  0.999561  0.902998  0.871795\n",
       "29   0.99969  0.999544  0.903461  0.858696"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow the example in Assignment 3 to randomize initialization:\n",
    "# Must complete this cell, includng output the df table that shows the accuracy and F1 scores\n",
    "# for training and validation for each iteration!\n",
    "# iterate through alpha in a range and collect scores, and print at the end. \n",
    "alpha_value=0.01\n",
    "hidden_layer=[32, 16, 8, 4]\n",
    "df_accuracy=pd.DataFrame([], index=range(10), columns=['Acc_tra','Acc_val','F1_tra','F1_val'])\n",
    "for i in range(30):\n",
    "    # complete the code to fit MLPClassifier in this for loop\n",
    "    nn_06 = MLPClassifier(solver='adam', activation='tanh', \n",
    "                      max_iter=1000, alpha=alpha_value, random_state=i, \n",
    "                      hidden_layer_sizes=hidden_layer).fit(Xtra, ytra) \n",
    "    # collect results, i.e. prediction scores\n",
    "    df_accuracy.loc[i,:]=[nn_06.score(Xtra,ytra), nn_06.score(Xval,yval), \n",
    "                          f1_score(nn_06.predict(Xtra),ytra)[0], f1_score(nn_06.predict(Xval),yval)[0]]\n",
    "df_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random state 16 is producing the highest F1 score of 90.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tuned Neural Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score (accuracy):  0.9997659217125168\n",
      "validation score (accuracy):  0.9996664442961974\n",
      "training score (F1, tp, pos):  (0.9288256227758007, 261, 295)\n",
      "validation score (F1, tp, pos):  (0.9025641025641026, 88, 99)\n"
     ]
    }
   ],
   "source": [
    "alpha_value=0.01\n",
    "random_seed=16\n",
    "hidden_layer=[32,16, 8, 4]\n",
    "nn_final = MLPClassifier(solver='adam', activation='tanh', \n",
    "                         max_iter=1000, alpha=alpha_value, random_state=random_seed, \n",
    "                         hidden_layer_sizes=hidden_layer).fit(Xtra, ytra)\n",
    "print('training score (accuracy): ', nn_final.score(Xtra,ytra))\n",
    "print('validation score (accuracy): ', nn_final.score(Xval,yval))\n",
    "print('training score (F1, tp, pos): ', f1_score(nn_final.predict(Xtra),ytra))\n",
    "print('validation score (F1, tp, pos): ', f1_score(nn_final.predict(Xval),yval))\n",
    "NeuralNet_Tuned_F1 = f1_score(nn_final.predict(Xval),yval)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selecting the Best Model </h2> <br>\n",
    "Whether it is a lonetree or a forest or logistic regression or a neural network. Select your final model based on the best F1 score on the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>CART(Lone Tree)</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Neural Network</th>\n",
       "      <th>Neural Network Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765714</td>\n",
       "      <td>0.782178</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.858639</td>\n",
       "      <td>0.902564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Logistic Regression  CART(Lone Tree)  Random Forest  Neural Network  \\\n",
       "0             0.765714         0.782178       0.875676        0.858639   \n",
       "\n",
       "   Neural Network Tuned  \n",
       "0              0.902564  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_results = pd.DataFrame(data = {\n",
    "    'Logistic Regression'   : logit_F1,\n",
    "    'CART(Lone Tree)'       : lonetree_F1,\n",
    "    'Random Forest'         : Random_Forest_F1,\n",
    "    'Neural Network'        : NeuralNet_F1,\n",
    "    'Neural Network Tuned'  : NeuralNet_Tuned_F1,\n",
    "    },index=[0])\n",
    "\n",
    "prediction_results\n",
    "#prediction_results.to_excel(excel_writer = 'linear_model_predictions.xlsx',\n",
    "#                            index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>My best model is Neural Network with an validation F1 score of 90.2<strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Demonstrating the Final Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your FINAL model, and only the FINAL model, using the test dataset\n",
    "* Note: it is a good idea to refit your final model on the TRAINING data, one last time (with the best parameters, if applicable.)\n",
    "* Once fit, you should print out the accuracy, F1 scores for training, validation, and test data.\n",
    "* Follow the example in Assignment 3 (my completed version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset % of Postive:  0.17\n",
      "training score (accuracy, %):   99.98\n",
      "training score (F1, tp, pos):   (0.9288256227758007, 261, 295)\n",
      " \n",
      "Validation Dataset % of Postive:  0.17\n",
      "validation score (accuracy, %):   99.97\n",
      "validation score (F1, tp, pos):   (0.9025641025641026, 88, 99)\n",
      " \n",
      "Test Dataset % of Postive:  0.17\n",
      "test score (accuracy, %):   99.94\n",
      "test score (F1, tp, pos):   (0.8241758241758241, 75, 98)\n"
     ]
    }
   ],
   "source": [
    "# Follow the example in Assignment 3 to refit and present the FINAL model:\n",
    "# Must complete this cell, includng printing the accuracy and F1 scores\n",
    "# for training, validation, and testing data sets.\n",
    "\n",
    "alpha_value=0.01\n",
    "random_seed=16\n",
    "hidden_layer=[32,16, 8, 4]\n",
    "nn_final = MLPClassifier(solver='adam', activation='tanh', \n",
    "                         max_iter=1000, alpha=alpha_value, random_state=random_seed, \n",
    "                         hidden_layer_sizes=hidden_layer).fit(Xtra, ytra) \n",
    "\n",
    "print('Training Dataset % of Postive: ', round(np.sum(ytra)/len(ytra)*100,2))\n",
    "print('training score (accuracy, %):  ', round(nn_final.score(Xtra,ytra)*100,2))\n",
    "print('training score (F1, tp, pos):  ', f1_score(nn_final.predict(Xtra),ytra))\n",
    "print(' ')  \n",
    "print('Validation Dataset % of Postive: ', round(np.sum(yval)/len(yval)*100,2))\n",
    "print('validation score (accuracy, %):  ', round(nn_final.score(Xval,yval)*100,2))\n",
    "print('validation score (F1, tp, pos):  ', f1_score(nn_final.predict(Xval),yval))\n",
    "print(' ')  \n",
    "print('Test Dataset % of Postive: ', round(np.sum(ytest)/len(ytest)*100,2))\n",
    "print('test score (accuracy, %):  ', round(nn_final.score(Xtest,ytest)*100,2))\n",
    "print('test score (F1, tp, pos):  ', f1_score(nn_final.predict(Xtest),ytest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color:red\">Our Final Neural Network model is giving us a F1 score of 82% when tested using test data </strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
